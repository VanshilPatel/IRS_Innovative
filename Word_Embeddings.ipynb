{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Word Embeddings \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13480,)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text=pd.read_csv('tokenized_text.csv')\n",
    "tokenized_text=tokenized_text.iloc[:,1].to_list()\n",
    "np.array(tokenized_text).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aaron' 'aarons' 'aasmashe' ... 'ﬂour' 'ﬂoure' 'ﬂuffy']\n",
      "(13480, 17087)\n",
      "17087\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(lowercase = True,\n",
    "                            ngram_range = (1,1))\n",
    "\n",
    "text_tfidf = vectorizer.fit_transform(tokenized_text)\n",
    "tfidf_words = vectorizer.get_feature_names_out()\n",
    "print(tfidf_words)\n",
    "print(text_tfidf.shape)\n",
    "print(len(tfidf_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Left Here>>>> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation as LDA\n",
    "\n",
    "lda = LDA(n_components = 50,\n",
    "          n_jobs = -1,\n",
    "          max_iter = 100)\n",
    "text_lda = lda.fit_transform(text_tfidf)\n",
    "print(text_lda.shape)\n",
    "\n",
    "\n",
    "nmf = NMF(alpha_W=0.0,\n",
    "         init='nndsvdar',\n",
    "         l1_ratio=0.0,\n",
    "         max_iter = 100,\n",
    "         n_components = 50,\n",
    "         solver='cd')\n",
    "\n",
    "text_nmf = nmf.fit_transform(text_tfidf)\n",
    "print(text_nmf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        Miso-Butter Roast Chicken With Acorn Squash Pa...\n",
      "1        Crispy Salt and Pepper Potatoes 2 large egg wh...\n",
      "2        Thanksgiving Mac and Cheese 1 cup evaporated m...\n",
      "3        Italian Sausage and Bread Stuffing 1 (¾- to 1-...\n",
      "4        Newton's Law 1 teaspoon dark brown sugar; 1 te...\n",
      "                               ...                        \n",
      "13475    Brownie Pudding Cake 1 cup all-purpose flour; ...\n",
      "13476    Israeli Couscous with Roasted Butternut Squash...\n",
      "13477    Rice with Soy-Glazed Bonito Flakes and Sesame ...\n",
      "13478    Spanakopita 1 stick (1/2 cup) plus 1 tablespoo...\n",
      "13479    Mexican Poblano, Spinach, and Black Bean \"Lasa...\n",
      "Name: All_text, Length: 13480, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# variable dependencies:\n",
    "\n",
    "text_series = pd.Series(pd.read_csv('All_text.csv')['All_text'])\n",
    "\n",
    "\n",
    "def docs_by_tops(top_mat, topic_range = (0,0), doc_range = (0,2)):\n",
    "    for i in range(topic_range[0], topic_range[1]):\n",
    "        topic_scores = pd.Series(top_mat[:,i])\n",
    "        doc_index = topic_scores.sort_values(ascending = False)[doc_range[0]:doc_range[1]].index\n",
    "        for j, index in enumerate(doc_index, doc_range[0]):\n",
    "            print('Topic #{}'.format(i),\n",
    "                  '\\nDocument #{}'.format(j),\n",
    "                  '\\nTopic Score: {}\\n\\n'.format(topic_scores[index]),\n",
    "                  text_series[index], '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_by_tops(text_lda,(0,3),(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_by_tops(text_nmf,(0,3),(0,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_by_tops(text_nmf,(1,2),(90000,90001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_nmf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for best topic words using cosine similarity\n",
    "# Variable Dependency:\n",
    "word_series = pd.Series(tfidf_words)\n",
    "\n",
    "def words_by_tops(tfidf_mat, top_mat, topic_range=(0,0), n_words=10):\n",
    "    topic_word_scores = tfidf_mat.T * top_mat\n",
    "    for i in range(topic_range[0],topic_range[1]):\n",
    "        word_scores = pd.Series(topic_word_scores[:,i])\n",
    "        word_index = word_scores.sort_values(ascending = False)[:n_words].index\n",
    "        print('\\nTopic #{}'.format(i))\n",
    "        for index in word_index:\n",
    "            print(word_series[index],'\\t\\t', word_scores[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords using LDA\n",
    "words_by_tops(text_tfidf, text_lda, (0,3), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words using NMF\n",
    "words_by_tops(text_tfidf, text_nmf, (0,3), 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pulling the top one-hundred documents ranked in similarity among Topic #1\n",
    "text_index = pd.Series(text_nmf[:,1]).sort_values(ascending = False)[:100].index\n",
    "text_4summary = pd.Series(pd.read_csv('Cleaned_Text.csv')['Cleaned_Text'])[text_index]\n",
    "\n",
    "# Manually Creating a list of recipe stop\n",
    "recipe_stopwords = ['cup','cups','ingredient','ingredients','teaspoon','tablespoon','oven']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating topic filter\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "parsed_texts = nlp(' '.join(text_4summary)) \n",
    "kw_filts = set([str(word) for word in parsed_texts \n",
    "                if (word.pos_== ('NOUN' or 'ADJ' or 'VERB'))\n",
    "                and str(word) not in recipe_stopwords])\n",
    "\n",
    "print('Execution Time: {} seconds', time.time() - start_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating adjecency Table for recipes.\n",
    "adjacency = pd.DataFrame(columns=kw_filts, index=kw_filts, data = 0)\n",
    "for i, word in enumerate(parsed_texts):\n",
    "    if any ([str(word) == item for item in kw_filts]):\n",
    "        end = min(len(parsed_texts), i+5) # Window of four words\n",
    "        nextwords = parsed_texts[i+1:end]\n",
    "        inset = [str(x) in kw_filts for x in nextwords]\n",
    "        neighbors = [str(nextwords[i]) for i in range(len(nextwords)) if inset[i]]\n",
    "        if neighbors:\n",
    "            adjacency.loc[str(word), neighbors] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set All Recommendation Model Parameters\n",
    "N_topics = 50             # Number of Topics to Extract from corpora\n",
    "N_top_docs = 200          # Number of top documents within each topic to extract keywords\n",
    "N_top_words = 25          # Number of keywords to extract from each topic\n",
    "N_docs_categorized = 2000 # Number of top documents within each topic to tag \n",
    "N_neighbor_window = 4     # Length of word-radius that defines the neighborhood for\n",
    "                          # each word in the TextRank adjacency table\n",
    "\n",
    "# Query Similarity Weights\n",
    "w_title = 0.2\n",
    "w_text = 0.3\n",
    "w_categories = 0.5\n",
    "w_array = np.array([w_title, w_text, w_categories])\n",
    "\n",
    "# Recipe Stopwords: for any high volume food recipe terminology that doesn't contribute\n",
    "# to the searchability of a recipe. This list must be manually created.\n",
    "recipe_stopwords = ['cup','cups','ingredient','ingredients','teaspoon','teaspoons','tablespoon',\n",
    "                   'tablespoons','C','F']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming Data Dependencies\n",
    "topic_transformed_matrix = text_nmf\n",
    "root_text_data = pd.read_csv('Cleaned_Text.csv')['Cleaned_Text.csv'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating  tags (keywords/categories) and assigning to corresponding documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "#recipes['tag_list'] = [[] for i in repeat(None, recipes.shape[0])]\n",
    "\n",
    "def topic_docs_4kwsummary(topic_document_scores, root_text_data):\n",
    "    '''Gathers and formats the top recipes in each topic'''\n",
    "    text_index = pd.Series(topic_document_scores).sort_values(ascending = False)[:N_top_docs].index\n",
    "    text_4kwsummary = pd.Series(root_text_data)[text_index]\n",
    "    return text_4kwsummary\n",
    "\n",
    "def generate_filter_kws(text_list):\n",
    "    '''Filters out specific parts of speech and stop words from the list of potential keywords'''\n",
    "    parsed_texts = nlp(' '.join(text_list)) \n",
    "    kw_filts = set([str(word) for word in parsed_texts \n",
    "                if (word.pos_== ('NOUN' or 'ADJ' or 'VERB'))\n",
    "                and word.lemma_ not in recipe_stopwords])\n",
    "    return list(kw_filts), parsed_texts\n",
    "\n",
    "def generate_adjacency(kw_filts, parsed_texts):\n",
    "    '''Tabulates counts of neighbors in the neighborhood window for each unique word'''\n",
    "    adjacency = pd.DataFrame(columns=kw_filts, index=kw_filts, data = 0)\n",
    "    for i, word in enumerate(parsed_texts):\n",
    "        if any ([str(word) == item for item in kw_filts]):\n",
    "            end = min(len(parsed_texts), i+N_neighbor_window+1) # Neighborhood Window Utilized Here\n",
    "            nextwords = parsed_texts[i+1:end]\n",
    "            inset = [str(x) in kw_filts for x in nextwords]\n",
    "            neighbors = [str(nextwords[i]) for i in range(len(nextwords)) if inset[i]]\n",
    "            if neighbors:\n",
    "                adjacency.loc[str(word), neighbors] += 1\n",
    "    return adjacency\n",
    "                \n",
    "def generate_wordranks(adjacency):\n",
    "    '''Runs TextRank on adjacency table'''\n",
    "    nx_words = nx.from_numpy_matrix(adjacency.values)\n",
    "    ranks=nx.pagerank(nx_words, alpha=.85, tol=.00000001)\n",
    "    return ranks\n",
    "\n",
    "def generate_tag_list(ranks):\n",
    "    '''Uses TextRank ranks to return actual key words for each topic in rank order'''\n",
    "    rank_values = [i for i in ranks.values()]\n",
    "    ranked = pd.DataFrame(zip(rank_values, list(kw_filts))).sort_values(by=0,axis=0,ascending=False)\n",
    "    kw_list = ranked.iloc[:N_top_words,1].to_list()\n",
    "    return kw_list\n",
    "\n",
    "# Master Function utilizing all above functions\n",
    "def generate_tags(topic_document_scores, root_text_data):\n",
    "    text_4kwsummary = topic_docs_4kwsummary(topic_document_scores, root_text_data)\n",
    "    kw_filts, parsed_texts = generate_filter_kws(text_4kwsummary)\n",
    "    adjacency = generate_adjacency(kw_filts, parsed_texts)\n",
    "    ranks = generate_wordranks(adjacency)\n",
    "    kw_list = generate_tag_list(ranks)\n",
    "    return kw_list\n",
    "\n",
    "def generate_kw_index(topic_document_scores):\n",
    "    kw_index = pd.Series(topic_document_scores).sort_values(ascending = False)[:N_docs_categorized].index\n",
    "    return kw_index\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Tags and distributing to relevant documents\n",
    "for i in range(topic_transformed_matrix.shape[1]):\n",
    "    scores = topic_transformed_matrix[:,i]\n",
    "    topic_kws = generate_tags(scores, root_text_data)\n",
    "    kw_index_4df = generate_kw_index(scores)\n",
    "    recipes.loc[kw_index_4df, 'tag_list'] += topic_kws\n",
    "    if i%10 == 0:\n",
    "        print('Topic #{} Checkpoint'.format(i))\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the precious dataframe so that I never have to calculate that again.\n",
    "recipes.to_csv('tagged_recipes_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = topic_transformed_matrix[:,1]\n",
    "topic_kws = generate_tags(scores, root_text_data)\n",
    "kw_index_4df = generate_kw_index(scores)\n",
    "recipes.loc[kw_index_4df, 'tag_list'] += topic_kws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipes.loc[:5,'tag_list']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
